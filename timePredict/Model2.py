import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os

class ModelTrainer:
    def __init__(self, feature_columns, target_column, model_path):
        """
        ä½¿ç”¨XGBoostçš„å¢é‡å­¦ä¹ æ¨¡å‹è®­ç»ƒå·¥å…·
        :param feature_columns: listï¼Œç‰¹å¾åˆ—å
        :param target_column: strï¼Œç›®æ ‡åˆ—å
        :param model_path: strï¼Œä¿å­˜æ¨¡å‹çš„è·¯å¾„
        """
        self.feature_columns = feature_columns
        self.target_column = target_column
        self.model_path = model_path
        self.best_model = None
        self.scaler = None
        self.best_mae = float('inf')
        self.best_r2 = 0

        os.makedirs(os.path.dirname(model_path),exist_ok=True)

        #å·²ç»æœ‰æ¨¡å‹äº†
        if os.path.exists(model_path):
            try:
                model_data = joblib.load(model_path)
                self.best_model = model_data['model']
                self.scaler = model_data['scaler']
                print(f"å·²ç»åŠ è½½äº†ç°æœ‰çš„æ¨¡å‹:{model_path}")
            except Exception as e:
                print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        
    def train_model(self,data,incremental= False):
        """
        è®­ç»ƒæ¨¡å‹
        :param data: pandas.DataFrame, åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„æ•°æ®
        :param incremental: bool, æ˜¯å¦å¢é‡è®­ç»ƒ
        :return: æ˜¯å¦è®­ç»ƒæˆåŠŸ
        """
        if len(data)<5:
            print("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
            return False
        
        X = data[self.feature_columns]
        y = data[self.target_column]

        # å¢é‡è®­ç»ƒåˆ†æ”¯
        if incremental and self.best_model is not None:
            print("è¿›è¡Œä¸€è½®å¢é‡è®­ç»ƒâ€¦â€¦")
            X_new = X
            y_new = y
        else:
            #é¦–æ¬¡è¿›è¡Œè®­ç»ƒ
            X_train,X_val,y_train,y_val = train_test_split(
                X,y,test_size=0.2,random_state=42
            )
            X_new = X_train
            y_new = y_train
            self.val_data = (X_val, y_val)  # ä¿å­˜éªŒè¯é›†ç”¨äºè¯„ä¼°
        
        if self.scaler is None:
            print("åˆ›å»ºæ–°çš„æ ‡å‡†åŒ–å™¨...")
            self.scaler = StandardScaler()
            X_new_scaled = self.scaler.fit_transform(X_new)
        else:
            print("ä½¿ç”¨ç°æœ‰æ ‡å‡†åŒ–å™¨...")
            X_new_scaled = self.scaler.transform(X_new)
        
        #è½¬æ¢æ•°æ®æ ¼å¼
        dtrain = xgb.DMatrix(X_new_scaled, label=y_new, feature_names=self.feature_columns)
        # å‡†å¤‡éªŒè¯é›† dval
        if hasattr(self, 'val_data'):
            X_val, y_val = self.val_data
            X_val_scaled = self.scaler.transform(X_val)
            dval = xgb.DMatrix(X_val_scaled, label=y_val, feature_names=self.feature_columns)
            eval_list = [(dval, "eval")]
        else:
            eval_list = []
        
        params = {
            "objective": "reg:squarederror",
            "learning_rate": 0.05,
            "max_depth": 4,
            "min_child_weight": 3,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "random_state": 42,
            "tree_method": "hist",
            "device": "cuda",
        }

        if incremental and self.best_model is not None:
            print("å¢é‡è®­ç»ƒç°æœ‰æ¨¡å‹...")
            self.best_model = xgb.train(
                params, dtrain,
                num_boost_round=50,
                evals=eval_list,
                early_stopping_rounds=10,
                xgb_model=self.best_model
            )
        else:
            print("å¼€å§‹æ–°æ¨¡å‹è®­ç»ƒ...")
            self.best_model = xgb.train(
                params, dtrain,
                num_boost_round=100,
                evals=eval_list,
                early_stopping_rounds=20
            )

        # è¯„ä¼°æ¨¡å‹
        if hasattr(self, 'val_data'):
            X_val, y_val = self.val_data
            X_val_scaled = self.scaler.transform(X_val)
            dval = xgb.DMatrix(X_val_scaled, feature_names=self.feature_columns)
            y_pred_val = self.best_model.predict(dval)
            current_mae, current_r2 = self._print_evaluation_metrics(y_val, y_pred_val, "éªŒè¯é›†")
            
            # æ›´æ–°æœ€ä½³æ¨¡å‹
            if current_mae < self.best_mae:
                self.best_mae = current_mae
                self.best_r2 = current_r2
                self._save_model()

        print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
        if hasattr(self, 'val_data'):
            print(f"æœ€ä½³éªŒè¯é›† MAE: {self.best_mae:.4f}")
            print(f"æœ€ä½³éªŒè¯é›† RÂ²: {self.best_r2:.4f}")
        return True
