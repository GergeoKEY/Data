import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os

class ModelTrainer:
    def __init__(self, feature_columns, target_column, model_path):
        """
        ä½¿ç”¨XGBoostçš„å¢é‡å­¦ä¹ æ¨¡å‹è®­ç»ƒå·¥å…·
        :param feature_columns: listï¼Œç‰¹å¾åˆ—å
        :param target_column: strï¼Œç›®æ ‡åˆ—å
        :param model_path: strï¼Œä¿å­˜æ¨¡å‹çš„è·¯å¾„
        """
        self.feature_columns = feature_columns
        self.target_column = target_column
        self.model_path = model_path
        self.best_model = None
        self.scaler = None
        self.best_mae = float('inf')
        self.best_r2 = 0
        self.checkpoint_dir = os.path.join(os.path.dirname(model_path), 'checkpoints')
        os.makedirs(self.checkpoint_dir, exist_ok=True)

    def train_model(self, data, incremental=False):
        """
        è®­ç»ƒæ¨¡å‹
        :param data: pandas.DataFrame, åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„æ•°æ®
        :param incremental: bool, æ˜¯å¦å¢é‡è®­ç»ƒ
        :return: æ˜¯å¦è®­ç»ƒæˆåŠŸ
        """
        if len(data) < 5:
            print("Not enough data to train the model.")
            return False

        # é¦–å…ˆåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_full, X_test, y_full, y_test = train_test_split(
            data[self.feature_columns], data[self.target_column], test_size=0.2, random_state=42
        )
        
        # ä»è®­ç»ƒé›†ä¸­å†åˆ’åˆ†å‡ºéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_full, y_full, test_size=0.2, random_state=42
        )

        # æ ‡å‡†åŒ–æ•°æ®
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)

        # è½¬æ¢ä¸ºDMatrixæ ¼å¼
        dtrain = xgb.DMatrix(X_train_scaled, label=y_train, feature_names=self.feature_columns)
        dval = xgb.DMatrix(X_val_scaled, label=y_val, feature_names=self.feature_columns)
        dtest = xgb.DMatrix(X_test_scaled, label=y_test, feature_names=self.feature_columns)

        # XGBoostå‚æ•° - é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
        params = {
            "objective": "reg:squarederror",
            "learning_rate": 0.05,
            "max_depth": 4,
            "min_child_weight": 3,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "random_state": 42,
            "tree_method": "hist",
            "device": "cuda",
        }

        # è®­ç»ƒå‚æ•°
        batch_size = 10
        initial_batch_size = 20
        num_batches = (len(X_train_scaled) - initial_batch_size) // batch_size

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.checkpoint_dir, 'latest_checkpoint.json')
        if os.path.exists(checkpoint_path) and incremental:
            print("Loading existing model checkpoint...")
            self.best_model = xgb.Booster()
            self.best_model.load_model(checkpoint_path)
        else:
            # åˆå§‹è®­ç»ƒ
            X_init = X_train_scaled[:initial_batch_size]
            y_init = y_train.iloc[:initial_batch_size]
            dtrain_init = xgb.DMatrix(X_init, label=y_init, feature_names=self.feature_columns)

            print(f"\nğŸ†• è®­ç»ƒåˆå§‹æ¨¡å‹ ({initial_batch_size} samples)...")
            self.best_model = xgb.train(
                params, dtrain_init, 
                num_boost_round=100,
                evals=[(dval, "eval")], 
                early_stopping_rounds=20
            )

        # è¯„ä¼°åˆå§‹æ¨¡å‹
        y_pred_val = self.best_model.predict(dval)
        y_pred_test = self.best_model.predict(dtest)
        self._print_evaluation_metrics(y_val, y_pred_val, "åˆå§‹æ¨¡å‹éªŒè¯é›†")
        self._print_evaluation_metrics(y_test, y_pred_test, "åˆå§‹æ¨¡å‹æµ‹è¯•é›†")

        # å¢é‡è®­ç»ƒ
        for i in range(1, num_batches + 1):
            current_size = initial_batch_size + i * batch_size
            X_batch = X_train_scaled[:current_size]
            y_batch = y_train.iloc[:current_size]
            dtrain_batch = xgb.DMatrix(X_batch, label=y_batch, feature_names=self.feature_columns)

            # è°ƒæ•´å­¦ä¹ ç‡
            learning_rate = 0.05 * (0.95 ** i)
            params["learning_rate"] = learning_rate
            
            print(f"\nğŸ”„ å¢é‡è®­ç»ƒ batch {i} ({current_size} samples, lr={learning_rate:.4f})...")
            self.best_model = xgb.train(
                params, dtrain_batch, 
                num_boost_round=50,  # å‡å°‘æ¯è½®çš„è®­ç»ƒè½®æ•°
                evals=[(dval, "eval")], 
                early_stopping_rounds=10,
                xgb_model=self.best_model
            )
            
            # è¯„ä¼°å½“å‰æ¨¡å‹
            y_pred_val = self.best_model.predict(dval)
            y_pred_test = self.best_model.predict(dtest)
            current_mae_val, current_r2_val = self._print_evaluation_metrics(y_val, y_pred_val, f"Batch {i} éªŒè¯é›†")
            current_mae_test, current_r2_test = self._print_evaluation_metrics(y_test, y_pred_test, f"Batch {i} æµ‹è¯•é›†")
            
            # æ›´æ–°æœ€ä½³æ¨¡å‹
            if current_mae_val < self.best_mae:
                self.best_mae = current_mae_val
                self.best_r2 = current_r2_val
                self._save_model()
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.best_model.save_model(checkpoint_path)

        print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯é›† MAE: {self.best_mae:.4f}")
        print(f"æœ€ä½³éªŒè¯é›† RÂ²: {self.best_r2:.4f}")
        return True

    def _print_evaluation_metrics(self, y_true, y_pred, stage_name=""):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        print(f"\n{stage_name}è¯„ä¼°ç»“æœ:")
        print(f"MAE: {mae:.4f}")
        print(f"MSE: {mse:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"RÂ²: {r2:.4f}")
        print(f"é¢„æµ‹å€¼èŒƒå›´: [{y_pred.min():.2f}, {y_pred.max():.2f}]")
        return mae, r2

    def _save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨"""
        model_data = {
            'model': self.best_model,
            'scaler': self.scaler
        }
        joblib.dump(model_data, self.model_path)

    def predict(self, X_new):
        """ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if self.best_model is None:
            model_data = joblib.load(self.model_path)
            self.best_model = model_data['model']
            self.scaler = model_data['scaler']
        
        # æ£€æŸ¥è¾“å…¥ç±»å‹å¹¶ç›¸åº”å¤„ç†
        if isinstance(X_new, pd.DataFrame):
            X_new = X_new[self.feature_columns]
        elif isinstance(X_new, np.ndarray):
            if X_new.ndim == 1:
                X_new = pd.DataFrame([X_new], columns=self.feature_columns)
            else:
                X_new = pd.DataFrame(X_new, columns=self.feature_columns)
        else:
            X_new = pd.DataFrame([X_new], columns=self.feature_columns)
            
        # åº”ç”¨æ ‡å‡†åŒ–
        X_new_scaled = self.scaler.transform(X_new)
        dtest = xgb.DMatrix(X_new_scaled, feature_names=self.feature_columns)
        return self.best_model.predict(dtest)[0]
